from collections.abc import Sequence
import ctypes as ct

import torch

from bitsandbytes.functional import get_ptr

from ..._ops import register_kernel
from ...cextension import lib
from ..utils import ipex_cpu

# torch._int_mm for s8@s8->s32 is supported on CPU from torch 2.4+.
# However, we can overflow if we use this without AVX512_VNNI support.
# This is fixed in torch 2.6+, so we set this as the minimum to be safe.
# For more information: https://github.com/pytorch/pytorch/pull/136942
# TODO(matthewdouglas): aarch64?
if torch.__version__ >= (2, 6):

    @register_kernel("bitsandbytes::int8_linear_matmul", "cpu")
    def _(A: torch.Tensor, B: torch.Tensor):
        return torch._int_mm(
            A.reshape(-1, A.shape[-1]),
            B.t(),
        ).reshape(*A.shape[:-1], B.shape[0])


@register_kernel("bitsandbytes::quantize_blockwise", "cpu")
def _(A: torch.Tensor, code: torch.Tensor, blocksize: int) -> tuple[torch.Tensor, torch.Tensor]:
    torch._check_is_size(blocksize)

    n = A.numel()

    # Only FP32 has c++ kernrl
    if A.dtype == torch.float32:
        blocks = -(n // -blocksize)

        absmax = torch.empty((blocks,), device=A.device, dtype=torch.float32)
        out = torch.empty_like(A, dtype=torch.uint8)

        lib.cquantize_blockwise_cpu_fp32(
            get_ptr(code),
            get_ptr(A),
            get_ptr(absmax),
            get_ptr(out),
            ct.c_longlong(blocksize),
            ct.c_longlong(n),
        )
    else:
        rem = n % blocksize
        has_rem = rem > 0
        blocks = n // blocksize + has_rem
        absmax = torch.zeros((blocks,), device=A.device, dtype=torch.float32)
        A_reshaped = A.reshape(n)
        A_com = A_reshaped[: n - rem]
        A_com_reshaped = A_com.reshape(n // blocksize, blocksize)
        absmax[: blocks - has_rem] = torch.abs(A_com_reshaped).max(dim=-1)[0]
        scaled_A = torch.clamp(A_com_reshaped * (1 / absmax[: blocks - has_rem].view(-1, 1)), -1, 1)
        scaled_A = scaled_A.reshape(-1)
        if has_rem:
            absmax[-1] = torch.abs(A_reshaped[n - rem :]).max()
            scaled_A_rem = torch.clamp(A_reshaped[n - rem :] * (1 / absmax[-1]), -1, 1)
            scaled_A = torch.cat([scaled_A, scaled_A_rem], dim=0)

        diff = torch.abs(scaled_A.unsqueeze(-1) - code.to(scaled_A.device))
        out = torch.argmin(diff, dim=-1).to(torch.uint8).to(scaled_A.device).reshape(A.shape)

    return out, absmax


@register_kernel("bitsandbytes::dequantize_blockwise", "cpu")
def _(A: torch.Tensor, absmax: torch.Tensor, code: torch.Tensor, blocksize: int, dtype: torch.dtype) -> torch.Tensor:
    torch._check_is_size(blocksize)
    torch._check(A.dtype == torch.uint8, lambda: f"A must be uint8, got {A.dtype}")

    # Only FP32 has c++ kernrl
    if dtype == torch.float32:
        out = torch.empty_like(A, dtype=dtype)

        lib.cdequantize_blockwise_cpu_fp32(
            get_ptr(code),
            get_ptr(A),
            get_ptr(absmax),
            get_ptr(out),
            ct.c_longlong(blocksize),
            ct.c_longlong(A.numel()),
        )
    else:
        out = code[A.reshape(-1).int()]
        blocks = out.shape[-1] // blocksize
        res = out.shape[-1] % blocksize
        if res != 0:
            out = torch.nn.functional.pad(out, (0, blocksize - res), mode="constant", value=0)
        out = (out.view(-1, blocksize) * absmax.view(-1, 1)).to(dtype).reshape(-1)
        out = out[: blocks * blocksize + res]
        out = out.reshape(A.shape)

    return out


if ipex_cpu:
    from bitsandbytes.utils import _reverse_4bit_compress_format

    @register_kernel("bitsandbytes::dequantize_nf4_ipex", "cpu")
    def _(
        A: torch.Tensor,
        absmax: torch.Tensor,
        blocksize: int,
        shape: Sequence[int],
        dtype: torch.dtype,
    ) -> torch.Tensor:
        ipex_weight = torch.ops.ipex_prepack.woq_linear_unpack_weight(A, "nf4", shape, 2)
        A = _reverse_4bit_compress_format(ipex_weight.reshape(-1)).reshape(1, -1)
        return torch.ops.bitsandbytes.dequantize_4bit.default(
            A,
            absmax,
            blocksize,
            "nf4",
            shape,
            dtype,
        )
