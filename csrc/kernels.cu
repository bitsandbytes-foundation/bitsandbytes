// Copyright (c) Facebook, Inc. and its affiliates.
//
// This source code is licensed under the MIT license found in the
// LICENSE file in the root directory of this source tree.

#include "common.cuh"
#include "kernels.cuh"
#include <cub/block/block_discontinuity.cuh>
#include <cub/block/block_load.cuh>
#include <cub/block/block_radix_sort.cuh>
#include <cub/block/block_reduce.cuh>
#include <cub/block/block_store.cuh>
#include <cub/cub.cuh>
#include <cub/warp/warp_reduce.cuh>
#include <cuda_fp16.h>
#include <math_constants.h>
#include <mma.h>

#if CCCL_VERSION >= 2008002
#include <cuda/std/functional>
#define CUB_REDUCTIONOP_MAX                                                                                            \
    cuda::maximum<> {}
#else
#define CUB_REDUCTIONOP_MAX cub::Max()
#endif

#define HLF_MAX 65504
#define TH 1024
#define NUM 4
#define NUM_BLOCK 4096

__device__ static float fp4_dequantization_lut[8] = {
    0.0f,            // 0b000
    0.005208333333f, // 0b001
    0.66666667f,     // 0b010
    1.0f,            // 0b011
    0.33333333f,     // 0b100
    0.5f,            // 0b101
    0.16666667f,     // 0b110
    0.25f            // 0b111
};

__device__ static float nf4_dequantization_lut[16] = {
    -1.0f,                 // 0b0000
    -0.6961928009986877f,  // 0b0001
    -0.5250730514526367f,  // 0b0010
    -0.39491748809814453f, // 0b0011
    -0.28444138169288635f, // 0b0100
    -0.18477343022823334f, // 0b0101
    -0.09105003625154495f, // 0b0110
    0.0f,                  // 0b0111
    0.07958029955625534f,  // 0b1000
    0.16093020141124725f,  // 0b1001
    0.24611230194568634f,  // 0b1010
    0.33791524171829224f,  // 0b1011
    0.44070982933044434f,  // 0b1100
    0.5626170039176941f,   // 0b1101
    0.7229568362236023f,   // 0b1110
    1.0f                   // 0b1111
};

// source: https://stackoverflow.com/questions/17399119/how-do-i-use-atomicmax-on-floating-point-values-in-cuda
__device__ float atomicMax(float* address, float val) {
    int* address_as_i = reinterpret_cast<int*>(address);
    int old = *address_as_i, assumed;
    do {
        assumed = old;
        old = atomicCAS(reinterpret_cast<int*>(address), assumed, __float_as_int(fmaxf(val, __int_as_float(assumed))));
    } while (assumed != old);
    return __int_as_float(old);
}

__device__ __forceinline__ float dDequantizeFP4Tree(unsigned char val) {
    float sign = 1.0f - 2 * ((val & 0b1000) >> 3);
    return fp4_dequantization_lut[val & 0b111] * sign;
}

__device__ unsigned char dQuantizeFP4(float x) {
    // FP4 with bias of 3
    // first bit is a sign
    // subnormals
    // 0b000 = 0
    // 0b001 = 0.0625
    // 0b110 = 2
    // 0b111 = 3
    // 0b100 = 4
    // 0b101 = 6
    // 0b010 = 8
    // 0b011 = 12

    // we do a binary search
    // the pivots are divided by 12 (the FP4 absmax)
    // since we assume input data is in [-1.0, 1.0]

    // !be careful here, its easy to make a mistake
    // that is difficult to notice if you add an extra
    // zero somewhere!

    int sign = x < 0 ? 0b1000 : 0b0000;
    x = fabsf(x);
    if (x > 0.29166667f)
        if (x > 0.583333f)
            if (x > 0.8333333f)
                return 0b0011 + sign;
            else
                return 0b0010 + sign;
        else if (x > 0.4166667f)
            return 0b101 + sign;
        else
            return 0b100 + sign;
    else if (x > 0.0859375f)
        if (x > 0.20833333f)
            return 0b0111 + sign;
        else
            return 0b0110 + sign;
    else if (x > 0.00260417f)
        return 0b0001 + sign;
    else
        return 0b0000 + sign;
}

__device__ __forceinline__ float dDequantizeNF4(unsigned char val) { return nf4_dequantization_lut[val & 0x0F]; }

__device__ unsigned char dQuantizeNF4(float x) {

    // the values for this tree was generated by test_normal_map_tree
    // in the file tests/test_functional.py
    if (x > 0.03979014977812767f)
        if (x > 0.3893125355243683f)         // 1
            if (x > 0.6427869200706482f)     // 11
                if (x > 0.8614784181118011f) // 111
                    return 0b1111;
                else
                    return 0b1110;
            else if (x > 0.5016634166240692f) // 110
                return 0b1101;
            else
                return 0b1100;
        else if (x > 0.2035212516784668f) // 10
            if (x > 0.2920137718319893f)  // 101
                return 0b1011;
            else
                return 0b1010;
        else if (x > 0.1202552504837513f) // 100
            return 0b1001;
        else
            return 0b1000;
    else if (x > -0.33967943489551544f)     // 0
        if (x > -0.13791173323988914f)      // 01
            if (x > -0.045525018125772476f) // 011
                return 0b0111;
            else
                return 0b0110;
        else if (x > -0.23460740596055984f) // 010
            return 0b0101;
        else
            return 0b0100;
    else if (x > -0.6106329262256622f) // 00
        if (x > -0.4599952697753906f)  // 001
            return 0b0011;
        else
            return 0b0010;
    else if (x > -0.8480964004993439f) // 000
        return 0b0001;
    else
        return 0b0000;
}

// sign function for lion
// taken from https://stackoverflow.com/a/4609795, but not sure if there's a proper way to do this in CUDA

template <typename T> __device__ int sgn(T val) { return (T(0) < val) - (val < T(0)); }

template <int STOCHASTIC> __device__ unsigned char dQuantize(float* smem_code, const float rand, float x) {
    int pivot = 127;
    int upper_pivot = 255;
    int lower_pivot = 0;

    float lower = -1.0f;
    float upper = 1.0f;

    float val = smem_code[pivot];
    // i>>=1 = {32, 16, 8, 4, 2, 1}
    for (int i = 64; i > 0; i >>= 1) {
        if (x > val) {
            lower_pivot = pivot;
            lower = val;
            pivot += i;
        } else {
            upper_pivot = pivot;
            upper = val;
            pivot -= i;
        }
        val = smem_code[pivot];
    }

    if (upper_pivot == 255)
        upper = smem_code[upper_pivot];
    if (lower_pivot == 0)
        lower = smem_code[lower_pivot];

    if (!STOCHASTIC) {
        if (x > val) {
            float midpoint = (upper + val) * 0.5f;
            if (x > midpoint) {
                return upper_pivot;
            } else
                return pivot;
        } else {
            float midpoint = (lower + val) * 0.5f;
            if (x < midpoint)
                return lower_pivot;
            else
                return pivot;
        }
    } else {
        if (x > val) {
            float dist_to_upper = fabsf(upper - x);
            float dist_full = upper - val;
            if (rand >= dist_to_upper / dist_full)
                return upper_pivot;
            else
                return pivot;
        } else {
            float dist_to_lower = fabsf(lower - x);
            float dist_full = val - lower;
            if (rand >= dist_to_lower / dist_full)
                return lower_pivot;
            else
                return pivot;
        }
    }
}

template <int SIGNED>
__device__ __forceinline__ unsigned char
    quantize_2D(float* __restrict__ quadrants, float* __restrict__ const smem_code, float x) {
    int pivot = 127;
    int upper_pivot = 255;
    int lower_pivot = 0;

    float lower = SIGNED ? -1.0f : 0.0f;
    float upper = 1.0f;
    float midpoint;
    float val = quadrants[1];
    int local_pivot = 1;
    int offset = 1;

    // i>>=1 = {32, 16, 8, 4, 2, 1}
    for (int i = 64; i > 0; i >>= 1) {
        if (x > val) {
            lower_pivot = pivot;
            lower = val;
            pivot += i;
            // val = i == 64 ? quadrants[2] : smem_code[pivot];
            local_pivot += offset;
        } else {
            upper_pivot = pivot;
            upper = val;
            pivot -= i;
            // val = i == 64 ? quadrants[0] : smem_code[pivot];
            local_pivot -= offset;
        }
        val = i >= 64 ? quadrants[local_pivot] : smem_code[pivot];
        offset -= 1;
    }

    if (x > val) {
        midpoint = (upper + val) * 0.5f;
        if (x > midpoint)
            return upper_pivot;
        else
            return pivot;
    } else {
        midpoint = (lower + val) * 0.5f;
        if (x < midpoint)
            return lower_pivot;
        else
            return pivot;
    }
}

template <typename T, int BLOCK_SIZE, int NUM_PER_TH, int STOCHASTIC, int DATA_TYPE>
//__launch_bounds__(TH, 4)
__global__ void kQuantizeBlockwise(
    float* code, T* __restrict__ const A, float* absmax, unsigned char* out, float* __restrict__ const rand,
    const int rand_offset, const int n
) {
    // This can overflow, so we clamp to INT32_MAX. We won't have more elements than this.
    const int n_full = min(gridDim.x * BLOCK_SIZE, INT32_MAX);

    const int base_idx = blockIdx.x * BLOCK_SIZE;
    int valid_items = 0;

    T vals[NUM_PER_TH];
    float rand_vals[NUM_PER_TH];
    unsigned char qvals[(DATA_TYPE > 0) ? NUM_PER_TH / 2 : NUM_PER_TH];

    float local_abs_max = 0.0f;
    int local_rand_idx = 0;

    typedef cub::BlockLoad<T, BLOCK_SIZE / NUM_PER_TH, NUM_PER_TH, cub::BLOCK_LOAD_WARP_TRANSPOSE> LoadT;
    typedef cub::BlockStore<
        unsigned char, BLOCK_SIZE / NUM_PER_TH, (DATA_TYPE > 0) ? NUM_PER_TH / 2 : NUM_PER_TH,
        cub::BLOCK_STORE_WARP_TRANSPOSE>
        StoreChar;
    typedef cub::BlockReduce<float, BLOCK_SIZE / NUM_PER_TH> BlockReduce;
    typedef cub::BlockLoad<float, BLOCK_SIZE / NUM_PER_TH, NUM_PER_TH, cub::BLOCK_LOAD_WARP_TRANSPOSE> LoadFloat;

    __shared__ typename LoadT::TempStorage loadt;
    __shared__ typename LoadFloat::TempStorage loadf;
    __shared__ typename StoreChar::TempStorage storec;
    __shared__ typename BlockReduce::TempStorage reduce;
    __shared__ float smem_code[256];
    __shared__ float smem_absmax_value[1];

    if (DATA_TYPE == General8bit)
        for (int i = threadIdx.x; i < 256; i += blockDim.x)
            smem_code[i] = code[i];

    for (int64_t i = base_idx; i < n_full; i += gridDim.x * BLOCK_SIZE) {
        valid_items = min(BLOCK_SIZE, static_cast<int>(n - i));
        local_abs_max = -FLT_MAX;

        __syncthreads();
        LoadT(loadt).Load(&(A[i]), vals, valid_items, (T)0.0f);

        // 1. compute local max
        // 2. broadcast local max
        // 3. normalize inputs and quantize

#pragma unroll NUM_PER_TH
        for (int j = 0; j < NUM_PER_TH; j++)
            local_abs_max = fmaxf(local_abs_max, fabsf((float)vals[j]));

        local_abs_max = BlockReduce(reduce).Reduce(local_abs_max, CUB_REDUCTIONOP_MAX, valid_items);

        if (threadIdx.x == 0) {
            smem_absmax_value[0] = 1.0f / local_abs_max;
            absmax[i / BLOCK_SIZE] = local_abs_max;
        }
        __syncthreads();

        local_abs_max = smem_absmax_value[0];

        if (STOCHASTIC) {
            local_rand_idx = ((blockIdx.x * NUM_BLOCK) + (threadIdx.x * NUM) + rand_offset) % (1024 - 4);
            LoadFloat(loadf).Load(&rand[local_rand_idx], rand_vals, BLOCK_SIZE, 0);
        }

        switch (DATA_TYPE) {
        case General8bit:
#pragma unroll NUM_PER_TH
            for (int j = 0; j < NUM_PER_TH; j++) {
                if (!STOCHASTIC)
                    qvals[j] = dQuantize<0>(smem_code, 0.0f, ((float)vals[j]) * local_abs_max);
                else
                    qvals[j] = dQuantize<1>(smem_code, rand_vals[j], ((float)vals[j]) * local_abs_max);
            }
            break;
        case FP4:
#pragma unroll NUM_PER_TH
            for (int j = 0; j < NUM_PER_TH / 2; j++) {
                qvals[j] = dQuantizeFP4(((float)vals[2 * j]) * local_abs_max) << 4;
                qvals[j] |= dQuantizeFP4(((float)vals[2 * j + 1]) * local_abs_max);
            }
            break;
        case NF4:
#pragma unroll NUM_PER_TH
            for (int j = 0; j < NUM_PER_TH / 2; j++) {
                qvals[j] = dQuantizeNF4(((float)vals[2 * j]) * local_abs_max) << 4;
                qvals[j] |= dQuantizeNF4(((float)vals[2 * j + 1]) * local_abs_max);
            }
            break;
        }

        __syncthreads();
        StoreChar(storec).Store(
            &(out[(DATA_TYPE > 0) ? i / 2 : i]), qvals, (DATA_TYPE > 0) ? (valid_items + 1) / 2 : valid_items
        );
    }
}

// Specialized kernel for blocksize=32 with 4-bit quantization
// Processes 2 blocks of 32 values per warp to maintain full thread utilization
// Uses 32 threads total: threads 0-15 handle block 0, threads 16-31 handle block 1
template <typename T, int DATA_TYPE>
__global__ void kQuantizeBlockwise32(
    float* code, T* __restrict__ const A, float* absmax, unsigned char* out, float* __restrict__ const rand,
    const int rand_offset, const int n
) {
    constexpr int BLOCK_SIZE = 32;        // Size of each quantization block
    constexpr int NUM_PER_TH = 2;         // Values per thread (for 4-bit packing)
    constexpr int THREADS = 32;           // Total threads (full warp)
    constexpr int THREADS_PER_BLOCK = 16; // Threads handling each quantization block

    const int base_idx = blockIdx.x * BLOCK_SIZE * 2; // 2 blocks per CUDA block

    T vals[NUM_PER_TH];
    unsigned char qvals[NUM_PER_TH / 2]; // For 4-bit: 2 values per byte
    float local_abs_max = 0.0f;

    const int block_id = threadIdx.x / THREADS_PER_BLOCK;        // 0 for threads 0-15, 1 for threads 16-31
    const int local_thread_id = threadIdx.x % THREADS_PER_BLOCK; // Thread ID within the block (0-15)

    typedef cub::BlockLoad<T, THREADS, NUM_PER_TH, cub::BLOCK_LOAD_WARP_TRANSPOSE> LoadT;
    typedef cub::BlockStore<unsigned char, THREADS, NUM_PER_TH / 2, cub::BLOCK_STORE_WARP_TRANSPOSE> StoreChar;
    typedef cub::WarpReduce<float, 16>
        WarpReduce; // Logical warp size of 16: threads 0-15 and 16-31 reduce independently

    __shared__ typename LoadT::TempStorage loadt;
    __shared__ typename StoreChar::TempStorage storec;
    __shared__ typename WarpReduce::TempStorage warp_reduce[2]; // One per logical warp
    __shared__ float smem_absmax_value[2];

    const int i = base_idx + block_id * BLOCK_SIZE;
    // Use a flag instead of early return: BlockLoad/BlockStore/__syncthreads are cooperative
    // operations that require ALL 32 threads to participate
    const bool block_valid = (i < n);

    // All 32 threads participate in the load (out-of-bounds threads get 0.0f)
    __syncthreads();
    LoadT(loadt).Load(&(A[base_idx]), vals, min(BLOCK_SIZE * 2, n - base_idx), (T)0.0f);

    // Each thread computes max of its values
    local_abs_max = -FLT_MAX;
#pragma unroll NUM_PER_TH
    for (int j = 0; j < NUM_PER_TH; j++)
        local_abs_max = fmaxf(local_abs_max, fabsf((float)vals[j]));

    // Reduce within each logical warp of 16 threads independently
    local_abs_max = WarpReduce(warp_reduce[block_id]).Reduce(local_abs_max, CUB_REDUCTIONOP_MAX);

    if (local_thread_id == 0) {
        if (block_valid) {
            smem_absmax_value[block_id] = 1.0f / local_abs_max;
            absmax[blockIdx.x * 2 + block_id] = local_abs_max;
        } else {
            smem_absmax_value[block_id] = 0.0f;
        }
    }
    __syncthreads();

    local_abs_max = smem_absmax_value[block_id];

    switch (DATA_TYPE) {
    case FP4:
#pragma unroll NUM_PER_TH
        for (int j = 0; j < NUM_PER_TH / 2; j++) {
            qvals[j] = dQuantizeFP4(((float)vals[2 * j]) * local_abs_max) << 4;
            qvals[j] |= dQuantizeFP4(((float)vals[2 * j + 1]) * local_abs_max);
        }
        break;
    case NF4:
#pragma unroll NUM_PER_TH
        for (int j = 0; j < NUM_PER_TH / 2; j++) {
            qvals[j] = dQuantizeNF4(((float)vals[2 * j]) * local_abs_max) << 4;
            qvals[j] |= dQuantizeNF4(((float)vals[2 * j + 1]) * local_abs_max);
        }
        break;
    }

    // All 32 threads participate in the store (valid_items limits the actual writes)
    __syncthreads();
    StoreChar(storec).Store(&(out[base_idx / 2]), qvals, min((BLOCK_SIZE * 2 + 1) / 2, (n - base_idx + 1) / 2));
}

template <typename T, int TILE_SIZE, int THREADS, int NUM_PER_TH, int DATA_TYPE>
__global__ void
    kDequantizeBlockwise(float* code, unsigned char* A, float* absmax, T* out, const int blocksize, const int n) {

    const int n_load = (gridDim.x * TILE_SIZE);
    int valid_items_load = 0;
    int valid_items_store = 0;
    const int base_idx = (blockIdx.x * TILE_SIZE);

    T vals[NUM_PER_TH * ((DATA_TYPE > 0) ? 2 : 1)];
    unsigned char qvals[NUM_PER_TH];
    float local_abs_max = -FLT_MAX;

    typedef cub::BlockLoad<unsigned char, THREADS, NUM_PER_TH, cub::BLOCK_LOAD_WARP_TRANSPOSE> LoadChar;
    typedef cub::BlockStore<T, THREADS, NUM_PER_TH*((DATA_TYPE > 0) ? 2 : 1), cub::BLOCK_STORE_WARP_TRANSPOSE> StoreT;

    __shared__ typename LoadChar::TempStorage loadchar;
    __shared__ typename StoreT::TempStorage storet;

    for (int i = base_idx; i < n_load; i += gridDim.x * TILE_SIZE) {
        if (DATA_TYPE > 0) {
            // Cast n to int64_t to avoid overflow for large n
            valid_items_load = min(TILE_SIZE, static_cast<int>((static_cast<int64_t>(n) + 1) / 2) - i);
            valid_items_store = min(TILE_SIZE * 2, n - i * 2);
        } else {
            valid_items_load = min(TILE_SIZE, n - i);
            valid_items_store = valid_items_load;
        }

        // Since blocksize will always be a power-of-2, we avoid more expensive
        // division by the blocksize and instead use a shift operation.
        // This is equivalent to (i+threadId.x*NUM_PER_TH)/blocksize.
        local_abs_max = __ldg(&absmax[(i + threadIdx.x * NUM_PER_TH) >> (31 - __clz(blocksize))]);

        __syncthreads();
        LoadChar(loadchar).Load(&(A[i]), qvals, valid_items_load, 128);

        switch (DATA_TYPE) {
        case General8bit:
// load code through read-only cache via __ldg
#pragma unroll NUM_PER_TH
            for (int j = 0; j < NUM_PER_TH; j++)
                vals[j] = __ldg(&code[qvals[j]]) * local_abs_max;
            break;
        case FP4:
#pragma unroll NUM_PER_TH
            for (int j = 0; j < NUM_PER_TH; j++) {
                vals[j * 2] = dDequantizeFP4Tree(qvals[j] >> 4) * local_abs_max;
                vals[j * 2 + 1] = dDequantizeFP4Tree(qvals[j] & 0x0F) * local_abs_max;
            }
            break;
        case NF4:
#pragma unroll NUM_PER_TH
            for (int j = 0; j < NUM_PER_TH; j++) {
                vals[j * 2] = dDequantizeNF4(qvals[j] >> 4) * local_abs_max;
                vals[j * 2 + 1] = dDequantizeNF4(qvals[j] & 0x0F) * local_abs_max;
            }
            break;
        }

        __syncthreads();
        StoreT(storet).Store(&(out[(DATA_TYPE > 0) ? i * 2 : i]), vals, valid_items_store);
    }
}

template <typename T, int OPTIMIZER, int BLOCK_SIZE, int NUM_VALS>
__launch_bounds__(BLOCK_SIZE / NUM_VALS, 1) __global__ void kPreconditionOptimizer32bit2State(
    T* g, T* p, float* state1, float* state2, float* unorm, const float beta1, const float beta2, const float eps,
    const float weight_decay, const int step, const float lr, const float gnorm_scale, const int n
) {

    const int n_full = (BLOCK_SIZE * (n / BLOCK_SIZE)) + (n % BLOCK_SIZE == 0 ? 0 : BLOCK_SIZE);
    const int base_idx = (blockIdx.x * blockDim.x * NUM_VALS);
    int valid_items = 0;

    T g_vals[NUM_VALS];

    float s1_vals[NUM_VALS];
    float s2_vals[NUM_VALS];

    const float correction1 = 1.0f / (1.0f - powf(beta1, step));
    const float correction2 = 1.0f / (1.0f - powf(beta2, step));

    typedef cub::BlockLoad<T, BLOCK_SIZE / NUM_VALS, NUM_VALS, cub::BLOCK_LOAD_WARP_TRANSPOSE> Load;
    typedef cub::BlockLoad<float, BLOCK_SIZE / NUM_VALS, NUM_VALS, cub::BLOCK_LOAD_WARP_TRANSPOSE> LoadFloat;
    typedef cub::BlockReduce<float, BLOCK_SIZE / NUM_VALS> BlockReduce;

    __shared__ union {
        typename Load::TempStorage load;
        typename LoadFloat::TempStorage loadf;
        typename BlockReduce::TempStorage reduce;
    } temp_storage;

    for (unsigned int i = base_idx; i < n_full; i += gridDim.x * BLOCK_SIZE) {
        valid_items = n - i >= (BLOCK_SIZE) ? (BLOCK_SIZE) : n - i;

        __syncthreads();
        Load(temp_storage.load).Load(&(g[i]), g_vals, valid_items, 0.0f);
        __syncthreads();
        LoadFloat(temp_storage.loadf).Load(&(state1[i]), s1_vals, valid_items, 0.0f);
        __syncthreads();
        LoadFloat(temp_storage.loadf).Load(&(state2[i]), s2_vals, valid_items, 0.0f);

#pragma unroll NUM_VALS
        for (unsigned int j = 0; j < NUM_VALS; j++)
            g_vals[j] = gnorm_scale * ((float)g_vals[j]);

#pragma unroll NUM_VALS
        for (unsigned int j = 0; j < NUM_VALS; j++) {
            switch (OPTIMIZER) {
            case ADAM:
                s1_vals[j] = s1_vals[j] * beta1 + ((1.0f - beta1) * ((float)g_vals[j]));
                s2_vals[j] = s2_vals[j] * beta2 + ((1.0f - beta2) * (((float)g_vals[j]) * ((float)g_vals[j])));
                s1_vals[j] *= correction1;
                s2_vals[j] *= correction2;
                s1_vals[j] = s1_vals[j] / (sqrtf(s2_vals[j]) + eps); // update
                s1_vals[j] *= s1_vals[j];                            // update l2 norm (update*update)
                break;
            case ADEMAMIX:
                break;
            }
        }

#pragma unroll NUM_VALS - 1
        for (unsigned int j = 1; j < NUM_VALS; j++)
            s1_vals[0] += s1_vals[j];

        __syncthreads();
        s1_vals[0] = BlockReduce(temp_storage.reduce).Sum(s1_vals[0]);

        if (threadIdx.x == 0)
            atomicAdd(&unorm[0], s1_vals[0]);

        __syncwarp();
    }
}

#define NUM_PER_THREAD 4

template <typename T, int OPTIMIZER>
__launch_bounds__(TH, 1) __global__ void kOptimizer32bit2State(
    T* g, T* p, float* state1, float* state2, float* unorm, const float max_unorm, const float param_norm,
    const float beta1, const float beta2, const float beta3, const float alpha, const float eps,
    const float weight_decay, const int step, const float lr, const float gnorm_scale, const bool skip_zeros,
    const int n
) {

    const int n_full = ((TH * NUM_PER_THREAD) * (n / (TH * NUM_PER_THREAD))) +
                       (n % (TH * NUM_PER_THREAD) == 0 ? 0 : (TH * NUM_PER_THREAD));
    const int base_idx = (blockIdx.x * blockDim.x * NUM_PER_THREAD);
    int valid_items = 0;
    float update_scale = 0.0f;
    T g_vals[NUM_PER_THREAD];
    T p_vals[NUM_PER_THREAD];

    float s1_vals[NUM_PER_THREAD];
    float s2_vals[NUM_PER_THREAD];

    // AdEMAMix has an additional state buffer, which we packed
    // into state1. We need thread-local storage here for these.
    // TODO: Mark with [[maybe_unused]] after upgrade to min compiler.
    float s3_vals[NUM_PER_THREAD];

    const float correction1 = 1.0f - powf(beta1, step);
    const float correction2 = sqrtf(1.0f - powf(beta2, step));
    const float step_size = -lr * correction2 / correction1;

    if (max_unorm > 0.0f) {
        update_scale = max_unorm > 0.0f ? sqrtf(unorm[0]) : 1.0f;
        if (update_scale > max_unorm * param_norm) {
            update_scale = (max_unorm * param_norm) / update_scale;
        } else {
            update_scale = 1.0f;
        }
    } else {
        update_scale = 1.0f;
    }

    typedef cub::BlockLoad<T, TH, NUM_PER_THREAD, cub::BLOCK_LOAD_WARP_TRANSPOSE> Load;
    typedef cub::BlockStore<T, TH, NUM_PER_THREAD, cub::BLOCK_STORE_WARP_TRANSPOSE> Store;

    typedef cub::BlockLoad<float, TH, NUM_PER_THREAD, cub::BLOCK_LOAD_WARP_TRANSPOSE> LoadFloat;
    typedef cub::BlockStore<float, TH, NUM_PER_THREAD, cub::BLOCK_STORE_WARP_TRANSPOSE> StoreFloat;

    __shared__ union {
        typename Load::TempStorage load;
        typename Store::TempStorage store;
        typename LoadFloat::TempStorage loadf;
        typename StoreFloat::TempStorage storef;
    } temp_storage;

    for (unsigned int i = base_idx; i < n_full; i += gridDim.x * TH * NUM_PER_THREAD) {
        valid_items = n - i >= (TH * NUM_PER_THREAD) ? (TH * NUM_PER_THREAD) : n - i;

        __syncthreads();
        Load(temp_storage.load).Load(&(g[i]), g_vals, valid_items);
        __syncthreads();
        LoadFloat(temp_storage.loadf).Load(&(state1[i]), s1_vals, valid_items);
        __syncthreads();
        LoadFloat(temp_storage.loadf).Load(&(state2[i]), s2_vals, valid_items);
        __syncthreads();
        Load(temp_storage.load).Load(&(p[i]), p_vals, valid_items);

        // Load additional state1 data for AdEMAMix
        // TODO: Make constexpr after updating min compiler
        if (OPTIMIZER == ADEMAMIX) {
            __syncthreads();
            LoadFloat(temp_storage.loadf).Load(&(state1[n + i]), s3_vals, valid_items);
        }

#pragma unroll 4
        for (unsigned int j = 0; j < NUM_PER_THREAD; j++)
            g_vals[j] = gnorm_scale * ((float)g_vals[j]);

#pragma unroll 4
        for (unsigned int j = 0; j < NUM_PER_THREAD; j++) {
            switch (OPTIMIZER) {
            case ADEMAMIX:
                // m1 update: m1 = beta1 * m1 + (1-beta1) * g
                s1_vals[j] = (s1_vals[j] * beta1) + ((1.0f - beta1) * (float)g_vals[j]);

                // m2 update: m2 = m2 * beta3 + (1-beta3) * g
                s3_vals[j] = (s3_vals[j] * beta3) + ((1.0f - beta3) * (float)g_vals[j]);

                // nu update: nu = beta2 * nu + (1-beta2) * g^2
                s2_vals[j] = (s2_vals[j] * beta2) + ((1.0f - beta2) * (float)g_vals[j] * (float)g_vals[j]);

                p_vals[j] = (float)p_vals[j] - lr * (((s1_vals[j] / correction1) + (alpha * s3_vals[j])) /
                                                     ((sqrtf(s2_vals[j]) / correction2) + eps));

                if (weight_decay > 0.0f)
                    p_vals[j] = ((float)p_vals[j]) * (1.0f - (lr * weight_decay));

                break;
            case ADAM:

                if (!skip_zeros || (skip_zeros && ((float)g_vals[j] != 0.0f))) {
                    s1_vals[j] = s1_vals[j] * beta1 + ((1.0f - beta1) * ((float)g_vals[j]));
                    s2_vals[j] = s2_vals[j] * beta2 + ((1.0f - beta2) * (((float)g_vals[j]) * ((float)g_vals[j])));
                    p_vals[j] = ((float)p_vals[j]) +
                                (update_scale * step_size * (s1_vals[j] / (sqrtf(s2_vals[j]) + (eps * correction2))));

                    if (weight_decay > 0.0f)
                        p_vals[j] = ((float)p_vals[j]) * (1.0f - (lr * weight_decay));
                }
                break;
            }
        }

        __syncthreads();
        Store(temp_storage.store).Store(&(p[i]), p_vals, valid_items);
        __syncthreads();
        StoreFloat(temp_storage.storef).Store(&(state1[i]), s1_vals, valid_items);
        __syncthreads();
        StoreFloat(temp_storage.storef).Store(&(state2[i]), s2_vals, valid_items);

        if (OPTIMIZER == ADEMAMIX) {
            __syncthreads();
            StoreFloat(temp_storage.storef).Store(&(state1[n + i]), s3_vals, valid_items);
        }
    }
}

template <typename T, int OPTIMIZER, int BLOCK_SIZE, int NUM_VALS>
__launch_bounds__(BLOCK_SIZE / NUM_VALS, 1) __global__ void kPreconditionOptimizer32bit1State(
    T* g, T* p, float* state1, float* unorm, const float beta1, const float beta2, const float eps,
    const float weight_decay, const int step, const float lr, const float gnorm_scale, const int n
) {

    const int n_full = (BLOCK_SIZE * (n / BLOCK_SIZE)) + (n % BLOCK_SIZE == 0 ? 0 : BLOCK_SIZE);
    const int base_idx = (blockIdx.x * blockDim.x * NUM_VALS);
    int valid_items = 0;

    T g_vals[NUM_VALS];

    float s1_vals[NUM_VALS];

    typedef cub::BlockLoad<T, BLOCK_SIZE / NUM_VALS, NUM_VALS, cub::BLOCK_LOAD_WARP_TRANSPOSE> Load;
    typedef cub::BlockLoad<float, BLOCK_SIZE / NUM_VALS, NUM_VALS, cub::BLOCK_LOAD_WARP_TRANSPOSE> LoadFloat;
    typedef cub::BlockReduce<float, BLOCK_SIZE / NUM_VALS> BlockReduce;

    __shared__ union {
        typename Load::TempStorage load;
        typename LoadFloat::TempStorage loadf;
        typename BlockReduce::TempStorage reduce;
    } temp_storage;

    for (unsigned int i = base_idx; i < n_full; i += gridDim.x * BLOCK_SIZE) {
        valid_items = n - i >= (BLOCK_SIZE) ? (BLOCK_SIZE) : n - i;

        __syncthreads();
        Load(temp_storage.load).Load(&(g[i]), g_vals, valid_items, 0.0f);
        __syncthreads();
        LoadFloat(temp_storage.loadf).Load(&(state1[i]), s1_vals, valid_items, 0.0f);

#pragma unroll NUM_VALS
        for (unsigned int j = 0; j < NUM_VALS; j++)
            g_vals[j] = gnorm_scale * ((float)g_vals[j]);

#pragma unroll NUM_VALS
        for (unsigned int j = 0; j < NUM_VALS; j++) {
            switch (OPTIMIZER) {
            case MOMENTUM:
                if (step == 1)
                    s1_vals[j] = (float)g_vals[j]; // state update
                else
                    s1_vals[j] = s1_vals[j] * beta1 + ((float)g_vals[j]); // state update
                s1_vals[j] = s1_vals[j] * s1_vals[j];                     // update norm
                break;
            case LION:
                s1_vals[j] = s1_vals[j] * beta2 + ((1.0f - beta2) * (float)g_vals[j]); // state update
                break;
            case RMSPROP:
                s1_vals[j] =
                    s1_vals[j] * beta1 + ((1.0f - beta1) * ((float)g_vals[j]) * ((float)g_vals[j])); // state update
                s1_vals[j] = __fdividef((float)g_vals[j], sqrtf(s1_vals[j]) + eps);                  // update value
                s1_vals[j] = s1_vals[j] * s1_vals[j];                                                // update norm
                break;
            case ADAGRAD:
                s1_vals[j] = s1_vals[j] + ((float)g_vals[j]) * ((float)g_vals[j]);  // state update
                s1_vals[j] = __fdividef((float)g_vals[j], sqrtf(s1_vals[j]) + eps); // update value
                s1_vals[j] = s1_vals[j] * s1_vals[j];                               // update norm
                break;
            }
        }

#pragma unroll
        for (unsigned int j = 1; j < NUM_VALS; j++)
            s1_vals[0] += s1_vals[j];

        __syncthreads();
        s1_vals[0] = BlockReduce(temp_storage.reduce).Sum(s1_vals[0], valid_items);

        if (threadIdx.x == 0)
            atomicAdd(&unorm[0], s1_vals[0]);

        __syncwarp();
    }
}

template <typename T, int OPTIMIZER>
__launch_bounds__(TH, 1) __global__ void kOptimizer32bit1State(
    T* g, T* p, float* state1, float* unorm, const float max_unorm, const float param_norm, const float beta1,
    const float beta2, const float eps, const float weight_decay, const int step, const float lr,
    const float gnorm_scale, const bool skip_zeros, const int n
) {

    const int n_full = ((TH * NUM_PER_THREAD) * (n / (TH * NUM_PER_THREAD))) +
                       (n % (TH * NUM_PER_THREAD) == 0 ? 0 : (TH * NUM_PER_THREAD));
    const int base_idx = (blockIdx.x * blockDim.x * NUM_PER_THREAD);
    int valid_items = 0;
    float update_scale = 0.0f;

    if (max_unorm > 0.0f) {
        update_scale = max_unorm > 0.0f ? sqrtf(unorm[0]) : 1.0f;
        if (update_scale > max_unorm * param_norm + eps) {
            update_scale = (max_unorm * param_norm + eps) / update_scale;
        } else {
            update_scale = 1.0f;
        }
    } else {
        update_scale = 1.0f;
    }

    T g_vals[NUM_PER_THREAD];
    T p_vals[NUM_PER_THREAD];

    float s1_vals[NUM_PER_THREAD];

    typedef cub::BlockLoad<T, TH, NUM_PER_THREAD, cub::BLOCK_LOAD_WARP_TRANSPOSE> Load;
    typedef cub::BlockStore<T, TH, NUM_PER_THREAD, cub::BLOCK_STORE_WARP_TRANSPOSE> Store;

    typedef cub::BlockLoad<float, TH, NUM_PER_THREAD, cub::BLOCK_LOAD_WARP_TRANSPOSE> LoadFloat;
    typedef cub::BlockStore<float, TH, NUM_PER_THREAD, cub::BLOCK_STORE_WARP_TRANSPOSE> StoreFloat;

    __shared__ union {
        typename Load::TempStorage load;
        typename Store::TempStorage store;
        typename LoadFloat::TempStorage loadf;
        typename StoreFloat::TempStorage storef;
    } temp_storage;

    for (unsigned int i = base_idx; i < n_full; i += gridDim.x * TH * NUM_PER_THREAD) {
        valid_items = n - i >= (TH * NUM_PER_THREAD) ? (TH * NUM_PER_THREAD) : n - i;

        __syncthreads();
        Load(temp_storage.load).Load(&(g[i]), g_vals, valid_items);
        __syncthreads();
        LoadFloat(temp_storage.loadf).Load(&(state1[i]), s1_vals, valid_items);
        __syncthreads();
        Load(temp_storage.load).Load(&(p[i]), p_vals, valid_items);

#pragma unroll 4
        for (unsigned int j = 0; j < NUM_PER_THREAD; j++) {
            g_vals[j] = gnorm_scale * ((float)g_vals[j]);
            if (weight_decay > 0.0f)
                g_vals[j] = (float)g_vals[j] + (((float)p_vals[j]) * weight_decay);
        }

#pragma unroll 4
        for (unsigned int j = 0; j < NUM_PER_THREAD; j++) {
            if (!skip_zeros || (skip_zeros && ((float)g_vals[j] != 0.0f))) {
                switch (OPTIMIZER) {
                case MOMENTUM:
                    if (step == 1)
                        s1_vals[j] = (float)g_vals[j];
                    else
                        s1_vals[j] = s1_vals[j] * beta1 + ((float)g_vals[j]);

                    p_vals[j] = ((float)p_vals[j]) + update_scale * (-lr * (s1_vals[j]));
                    break;
                case LION:
                    p_vals[j] =
                        ((float)p_vals[j]) -
                        update_scale * (lr * sgn(((float)s1_vals[j]) * beta1 + ((1.0f - beta1) * ((float)g_vals[j]))));
                    s1_vals[j] = s1_vals[j] * beta2 + ((1.0f - beta2) * ((float)g_vals[j]));
                    break;
                case RMSPROP:
                    s1_vals[j] = s1_vals[j] * beta1 + ((1.0f - beta1) * ((float)g_vals[j]) * ((float)g_vals[j]));
                    p_vals[j] = ((float)p_vals[j]) -
                                update_scale * (lr * __fdividef((float)g_vals[j], sqrtf((float)s1_vals[j]) + eps));
                    break;
                case ADAGRAD:
                    s1_vals[j] = s1_vals[j] + ((float)g_vals[j]) * ((float)g_vals[j]);
                    p_vals[j] = ((float)p_vals[j]) - lr * __fdividef((float)g_vals[j], sqrtf((float)s1_vals[j]) + eps);
                    break;
                }
            }
        }

        __syncthreads();
        Store(temp_storage.store).Store(&(p[i]), p_vals, valid_items);
        __syncthreads();
        StoreFloat(temp_storage.storef).Store(&(state1[i]), s1_vals, valid_items);
    }
}

#define LANES 2
#define QUAD 3

template <typename T, int OPTIMIZER, int BLOCK_SIZE, int N_PER_TH>
__launch_bounds__(256, 3) __global__ void kOptimizerStatic8bit2StateBlockwise(
    T* p, T* __restrict__ const g, unsigned char* state1, unsigned char* state2, const float beta1, const float beta2,
    const float beta3, const float alpha, const float eps, const int step, const float lr,
    float* __restrict__ const quantiles1, float* __restrict__ const quantiles2, float* absmax1, float* absmax2,
    float weight_decay, const float gnorm_scale, const bool skip_zeros, const int n
) {

    // const int n_full = n + (n%BLOCK_SIZE);
    const int n_full = gridDim.x * BLOCK_SIZE;
    const int base_idx = (blockIdx.x * BLOCK_SIZE);
    int valid_items = 0;
    float g_val = 0.0f;
    float s1_vals[N_PER_TH];
    float s2_vals[N_PER_TH];
    float s3_vals[N_PER_TH];

    // 2-5%
    const float correction1 = 1.0f - __powf(beta1, step);
    const float correction2 = sqrtf(1.0f - __powf(beta2, step));
    const float step_size = __fdividef(-lr * correction2, correction1);
    const int lane_id = threadIdx.x % LANES;
    float new_local_abs_max1 = -FLT_MAX;
    float new_local_abs_max2 = -FLT_MAX;
    float new_local_abs_max3 = -FLT_MAX;
    float quadrants1[QUAD];
    float quadrants2[QUAD];

    unsigned char c1s[N_PER_TH];
    unsigned char c2s[N_PER_TH];
    unsigned char c3s[N_PER_TH];

    T g_vals[N_PER_TH];
    T p_vals[N_PER_TH];
    typedef cub::BlockLoad<T, BLOCK_SIZE / N_PER_TH, N_PER_TH, cub::BLOCK_LOAD_WARP_TRANSPOSE> LoadT;
    typedef cub::BlockLoad<unsigned char, BLOCK_SIZE / N_PER_TH, N_PER_TH, cub::BLOCK_LOAD_WARP_TRANSPOSE> LoadChar;

    typedef cub::BlockStore<unsigned char, BLOCK_SIZE / N_PER_TH, N_PER_TH, cub::BLOCK_STORE_WARP_TRANSPOSE> StoreChar;
    typedef cub::BlockStore<T, BLOCK_SIZE / N_PER_TH, N_PER_TH, cub::BLOCK_STORE_WARP_TRANSPOSE> StoreT;

    __shared__ float smem_quantiles1[LANES][257];
    __shared__ float smem_quantiles2[LANES][257];
    typedef cub::BlockReduce<float, BLOCK_SIZE / N_PER_TH> BlockReduce1;
    typedef cub::BlockReduce<float, BLOCK_SIZE / N_PER_TH> BlockReduce2;
    typedef cub::BlockReduce<float, BLOCK_SIZE / N_PER_TH> BlockReduce3;
    __shared__ typename BlockReduce1::TempStorage reduce1;
    __shared__ typename BlockReduce2::TempStorage reduce2;
    __shared__ typename BlockReduce2::TempStorage reduce3;
    __shared__ float smem_exchange1[1];
    __shared__ float smem_exchange2[1];
    __shared__ float smem_exchange3[1]; // [[maybe_unused]]

    __shared__ union {
        typename LoadT::TempStorage loadh;
        typename LoadChar::TempStorage loadc;
        typename StoreChar::TempStorage storec;
        typename StoreT::TempStorage storeh;
    } temp_storage;

    // init: 0.2 -> 0.23

    // 0.23 -> 0.23
    smem_quantiles1[0][threadIdx.x] = quantiles1[threadIdx.x];
    smem_quantiles2[0][threadIdx.x] = quantiles2[threadIdx.x];
#pragma unroll
    for (unsigned int j = 1; j < LANES; j++) {
        smem_quantiles1[j][threadIdx.x] = smem_quantiles1[0][threadIdx.x];
        smem_quantiles2[j][threadIdx.x] = smem_quantiles2[0][threadIdx.x];
    }

    __syncthreads();

#pragma unroll
    for (int k = 0; k < QUAD; k++) {
        quadrants1[k] = smem_quantiles1[lane_id][(k * 256 / (QUAD + 1)) + (256 / (QUAD + 1) - 1)];
        quadrants2[k] = smem_quantiles2[lane_id][(k * 256 / (QUAD + 1)) + (256 / (QUAD + 1) - 1)];
    }

    for (unsigned int i = base_idx; i < n_full; i += gridDim.x * BLOCK_SIZE) {
        // loads: 0.23 -> 0.85/1.44
        valid_items = n - i >= BLOCK_SIZE ? BLOCK_SIZE : n - i;
        __syncthreads();
        LoadT(temp_storage.loadh).Load(&(g[i]), g_vals, valid_items, (T)0.0f);
        __syncthreads();
        LoadChar(temp_storage.loadc).Load(&(state1[i]), c1s, valid_items, 128);
        __syncthreads();
        LoadChar(temp_storage.loadc).Load(&(state2[i]), c2s, valid_items, 0);

        // AdEMAMix has an additional state packed into state1.
        if (OPTIMIZER == ADEMAMIX) {
            __syncthreads();
            LoadChar(temp_storage.loadc).Load(&(state1[n + i]), c3s, valid_items, 128);
        }

        new_local_abs_max1 = -FLT_MAX;
        new_local_abs_max2 = -FLT_MAX;
        new_local_abs_max3 = -FLT_MAX;

//  update: 2.48/1.57 -> 2.51/1.60
#pragma unroll N_PER_TH
        for (unsigned int j = 0; j < N_PER_TH; j++) {
            if (!isnan((float)g_vals[j]) && !isinf((float)g_vals[j])) {
                s2_vals[j] = smem_quantiles2[lane_id][c2s[j]] * absmax2[i / BLOCK_SIZE];
                g_val = g_vals[j];
                // float ratio = (g_val*g_val)/fmaxf(s2_vals[j], eps*eps);
                // g_val = ratio > 2.0f ? 2.0f*g_val/ratio : g_val;
                g_val *= gnorm_scale;

                s2_vals[j] = (s2_vals[j] * beta2) + (((1.0f - beta2) * g_val * g_val));

                s1_vals[j] = smem_quantiles1[lane_id][c1s[j]] * absmax1[i / BLOCK_SIZE];
                s1_vals[j] = (s1_vals[j] * beta1) + (((1.0f - beta1) * g_val));

                if (OPTIMIZER == ADEMAMIX) {
                    // The absmax for the third state is appended to absmax1
                    s3_vals[j] = smem_quantiles1[lane_id][c3s[j]] * absmax1[(n + i) / BLOCK_SIZE];
                    s3_vals[j] = (s3_vals[j] * beta3) + (((1.0f - beta3) * g_val));
                }
            } else {
                s1_vals[j] = 0.0f;
                s2_vals[j] = 0.0f;

                if (OPTIMIZER == ADEMAMIX) {
                    s3_vals[j] = 0.0f;
                }
            }

            new_local_abs_max1 = fmaxf(new_local_abs_max1, fabsf(s1_vals[j]));
            new_local_abs_max2 = fmaxf(new_local_abs_max2, fabsf(s2_vals[j]));

            if (OPTIMIZER == ADEMAMIX) {
                new_local_abs_max3 = fmaxf(new_local_abs_max3, fabsf(s3_vals[j]));
            }
        }

        //  reduce: 2.51/1.60 -> 2.67/1.69
        new_local_abs_max1 = BlockReduce1(reduce1).Reduce(new_local_abs_max1, CUB_REDUCTIONOP_MAX);
        new_local_abs_max2 = BlockReduce2(reduce2).Reduce(new_local_abs_max2, CUB_REDUCTIONOP_MAX);

        if (OPTIMIZER == ADEMAMIX) {
            new_local_abs_max3 = BlockReduce3(reduce3).Reduce(new_local_abs_max3, CUB_REDUCTIONOP_MAX);
        }

        if (threadIdx.x == 0) {
            smem_exchange1[0] = new_local_abs_max1;
            smem_exchange2[0] = new_local_abs_max2;

            if (OPTIMIZER == ADEMAMIX) {
                smem_exchange3[0] = new_local_abs_max3;
            }
        }

        __syncthreads();

        if (threadIdx.x == 0) {
            absmax1[i / BLOCK_SIZE] = new_local_abs_max1;
            absmax2[i / BLOCK_SIZE] = new_local_abs_max2;

            if (OPTIMIZER == ADEMAMIX) {
                absmax1[(n + i) / BLOCK_SIZE] = new_local_abs_max3;
            }
        } else {
            new_local_abs_max1 = smem_exchange1[0];
            new_local_abs_max2 = smem_exchange2[0];

            if (OPTIMIZER == ADEMAMIX) {
                new_local_abs_max3 = smem_exchange3[0];
            }
        }

        __syncthreads();
        LoadT(temp_storage.loadh).Load(&(p[i]), p_vals, valid_items, (T)0.0f);
//  reduce: 2.67/1.69 -> 2.67/1.70
#pragma unroll N_PER_TH
        for (unsigned int j = 0; j < N_PER_TH; j++) {
            // if(!skip_zeros || (skip_zeros && ((float)g_vals[j] != 0.0f)))
            if (!isnan((float)g_vals[j]) && !isinf((float)g_vals[j])) {
                if (OPTIMIZER == ADEMAMIX) {
                    p_vals[j] =
                        T((float)p_vals[j] - lr * (((s1_vals[j] / correction1) + (alpha * s3_vals[j])) /
                                                   ((sqrtf(s2_vals[j]) / correction2) + eps)));
                } else {
                    p_vals[j] =
                        (T)(((float)p_vals[j]) +
                            ((step_size * (__fdividef(s1_vals[j], (sqrtf(s2_vals[j]) + (correction2 * eps)))))));
                }

                if (weight_decay > 0.0f)
                    p_vals[j] = ((float)p_vals[j]) * (1.0f - (lr * weight_decay));
            }
        }

        //  store: 0.85/1.44 -> 2.48/1.57
        __syncthreads();
        StoreT(temp_storage.storeh).Store(&(p[i]), p_vals, valid_items);

//  quantizaztion: 2.67/1.70  -> 3.4/3.3
#pragma unroll N_PER_TH
        for (unsigned int j = 0; j < N_PER_TH; j++) {
            c1s[j] = quantize_2D<1>(quadrants1, smem_quantiles1[lane_id], __fdividef(s1_vals[j], new_local_abs_max1));
            c2s[j] = quantize_2D<0>(quadrants2, smem_quantiles2[lane_id], __fdividef(s2_vals[j], new_local_abs_max2));

            // make sure state1 term has still the same sign after quantization
            // (not needed for state2 term which has only positive values)
            if (signbit(smem_quantiles1[lane_id][c1s[j]]) != signbit(s1_vals[j])) {
                if (s1_vals[j] > 0.0f)
                    c1s[j] += 1;
                else
                    c1s[j] -= 1;
            }

            if (OPTIMIZER == ADEMAMIX) {
                c3s[j] =
                    quantize_2D<1>(quadrants1, smem_quantiles1[lane_id], __fdividef(s3_vals[j], new_local_abs_max3));

                if (signbit(smem_quantiles1[lane_id][c3s[j]]) != signbit(s3_vals[j])) {
                    c3s[j] += (s3_vals[j] > 0.0f) ? 1 : -1;
                }
            }
        }

        __syncthreads();
        StoreChar(temp_storage.storec).Store(&(state1[i]), c1s, valid_items);
        __syncthreads();
        StoreChar(temp_storage.storec).Store(&(state2[i]), c2s, valid_items);

        if (OPTIMIZER == ADEMAMIX) {
            __syncthreads();
            StoreChar(temp_storage.storec).Store(&(state1[n + i]), c3s, valid_items);
        }
    }
}

#define LANES 2
#define QUAD 3

template <typename T, int OPTIMIZER, int BLOCK_SIZE, int N_PER_TH>
__launch_bounds__(256, 3) __global__ void kOptimizerStatic8bit1StateBlockwise(
    T* p, T* __restrict__ const g, unsigned char* state1, const float beta1, const float beta2, const float eps,
    const int step, const float lr, float* __restrict__ const quantiles1, float* absmax1, float weight_decay,
    const float gnorm_scale, const bool skip_zeros, const int n
) {

    // const int n_full = n + (n%BLOCK_SIZE);
    const int n_full = gridDim.x * BLOCK_SIZE;
    const int base_idx = (blockIdx.x * BLOCK_SIZE);
    int valid_items = 0;
    float g_val = 0.0f;
    float s1_vals[N_PER_TH];
    // 2-5%
    const int lane_id = threadIdx.x % LANES;
    float new_local_abs_max1 = -FLT_MAX;
    float quadrants1[QUAD];

    unsigned char c1s[N_PER_TH];
    T g_vals[N_PER_TH];
    T p_vals[N_PER_TH];

    typedef cub::BlockLoad<T, BLOCK_SIZE / N_PER_TH, N_PER_TH, cub::BLOCK_LOAD_WARP_TRANSPOSE> LoadT;
    typedef cub::BlockLoad<unsigned char, BLOCK_SIZE / N_PER_TH, N_PER_TH, cub::BLOCK_LOAD_WARP_TRANSPOSE> LoadChar;

    typedef cub::BlockStore<unsigned char, BLOCK_SIZE / N_PER_TH, N_PER_TH, cub::BLOCK_STORE_WARP_TRANSPOSE> StoreChar;
    typedef cub::BlockStore<T, BLOCK_SIZE / N_PER_TH, N_PER_TH, cub::BLOCK_STORE_WARP_TRANSPOSE> StoreT;

    __shared__ float smem_quantiles1[LANES][257];
    typedef cub::BlockReduce<float, BLOCK_SIZE / N_PER_TH> BlockReduce1;
    __shared__ typename BlockReduce1::TempStorage reduce1;
    __shared__ float smem_exchange1[1];

    __shared__ union {
        typename LoadT::TempStorage loadh;
        typename LoadChar::TempStorage loadc;
        typename StoreChar::TempStorage storec;
        typename StoreT::TempStorage storeh;
    } temp_storage;

    // init: 0.2 -> 0.23

    // 0.23 -> 0.23
    smem_quantiles1[0][threadIdx.x] = quantiles1[threadIdx.x];
#pragma unroll
    for (unsigned int j = 1; j < LANES; j++)
        smem_quantiles1[j][threadIdx.x] = smem_quantiles1[0][threadIdx.x];

    __syncthreads();

#pragma unroll
    for (int k = 0; k < QUAD; k++)
        quadrants1[k] = smem_quantiles1[lane_id][(k * 256 / (QUAD + 1)) + (256 / (QUAD + 1) - 1)];

    for (unsigned int i = base_idx; i < n_full; i += gridDim.x * BLOCK_SIZE) {
        // loads: 0.23 -> 0.85/1.44
        valid_items = n - i >= BLOCK_SIZE ? BLOCK_SIZE : n - i;
        __syncthreads();
        LoadT(temp_storage.loadh).Load(&(g[i]), g_vals, valid_items, (T)0.0f);
        __syncthreads();
        LoadChar(temp_storage.loadc).Load(&(state1[i]), c1s, valid_items, 128);
        __syncthreads();
        LoadT(temp_storage.loadh).Load(&(p[i]), p_vals, valid_items, (T)0.0f);

        new_local_abs_max1 = -FLT_MAX;

//  update: 2.48/1.57 -> 2.51/1.60
#pragma unroll N_PER_TH
        for (unsigned int j = 0; j < N_PER_TH; j++) {
            g_val = float(g_vals[j]);
            g_val *= gnorm_scale;
            if (!skip_zeros || (skip_zeros && ((float)g_vals[j] != 0.0f))) {
                if (weight_decay > 0.0f) {
                    switch (OPTIMIZER) {
                    case MOMENTUM:
                    case ADAGRAD:
                    case RMSPROP:
                        g_val += ((float)p_vals[j]) * weight_decay;
                        break;
                    case LION:
                        p_vals[j] = ((float)p_vals[j]) * (1.0f - lr * weight_decay);
                        break;
                    }
                }

                s1_vals[j] = smem_quantiles1[lane_id][c1s[j]] * absmax1[i / BLOCK_SIZE];

                switch (OPTIMIZER) {
                case MOMENTUM:
                    if (step == 1)
                        s1_vals[j] = g_val;
                    else
                        s1_vals[j] = (s1_vals[j] * beta1) + g_val;
                    break;
                case LION:
                    // here, using gvals[j] to store the gradient smoothed by beta1 for the following parameter update,
                    // before the momentum is updated by beta2
                    g_vals[j] = lr * sgn(((float)s1_vals[j]) * beta1 + ((1.0f - beta1) * g_val));
                    s1_vals[j] = s1_vals[j] * beta2 + ((1.0f - beta2) * g_val);
                    break;
                case RMSPROP:
                    s1_vals[j] = s1_vals[j] * beta1 + ((1.0f - beta1) * (g_val * g_val));
                    break;
                case ADAGRAD:
                    s1_vals[j] = s1_vals[j] + (g_val * g_val);
                    break;
                }
            }

            new_local_abs_max1 = fmaxf(new_local_abs_max1, fabsf(s1_vals[j]));
        }

        //  reduce: 2.51/1.60 -> 2.67/1.69
        new_local_abs_max1 = BlockReduce1(reduce1).Reduce(new_local_abs_max1, CUB_REDUCTIONOP_MAX);

        if (threadIdx.x == 0)
            smem_exchange1[0] = new_local_abs_max1;

        __syncthreads();

        if (threadIdx.x == 0)
            absmax1[i / BLOCK_SIZE] = new_local_abs_max1;
        else
            new_local_abs_max1 = smem_exchange1[0];

//  reduce: 2.67/1.69 -> 2.67/1.70
#pragma unroll N_PER_TH
        for (unsigned int j = 0; j < N_PER_TH; j++) {
            if (!skip_zeros || (skip_zeros && ((float)g_vals[j] != 0.0f))) {
                switch (OPTIMIZER) {
                case MOMENTUM:
                    p_vals[j] = ((float)p_vals[j]) - lr * (s1_vals[j]);
                    break;
                case LION:
                    p_vals[j] = ((float)p_vals[j]) - ((float)g_vals[j]);
                    break;
                case RMSPROP:
                    g_val = g_vals[j];
                    p_vals[j] = ((float)p_vals[j]) - lr * (__fdividef(g_val, sqrtf(s1_vals[j]) + eps));
                    break;
                case ADAGRAD:
                    g_val = g_vals[j];
                    p_vals[j] = ((float)p_vals[j]) - lr * (__fdividef(g_val, sqrtf(s1_vals[j]) + eps));
                    break;
                }
            }
        }

        //  store: 0.85/1.44 -> 2.48/1.57
        __syncthreads();
        StoreT(temp_storage.storeh).Store(&(p[i]), p_vals, valid_items);

//  quantizaztion: 2.67/1.70  -> 3.4/3.3
#pragma unroll N_PER_TH
        for (unsigned int j = 0; j < N_PER_TH; j++) {
            c1s[j] = quantize_2D<1>(quadrants1, smem_quantiles1[lane_id], __fdividef(s1_vals[j], new_local_abs_max1));

            // make sure state1 term has still the same sign after quantization
            // (not needed for state2 term which has only positive values)
            if (signbit(smem_quantiles1[lane_id][c1s[j]]) != signbit(s1_vals[j])) {
                if (s1_vals[j] > 0.0f)
                    c1s[j] += 1;
                else
                    c1s[j] -= 1;
            }
        }

        __syncthreads();
        StoreChar(temp_storage.storec).Store(&(state1[i]), c1s, valid_items);
    }
}

// Inputs:
//  A [rows, cols]
// Outputs:
//  rowStats [rows]
//  out [rows, cols]
template <typename T, int THREADS, int SPARSE_DECOMP>
__launch_bounds__(1024, BNB_MAX_THREADS_PER_SM / 1024) __global__
    void kInt8VectorQuant(T* __restrict__ A, int8_t* out, float* rowStats, float threshold, int rows, int cols) {

    using BlockReduceT = cub::BlockReduce<T, THREADS>;

    // One block per row.
    // Threads load column values in a striped arrangement.
    // e.g. t0 reads row[0], row[0+nthreads], ..
    // and  t1 reads row[1], row[1+nthreads], ..
    // Each thread will determine its local absmax.
    // We then do a blockwise reduction to determine the row's absmax.

    __shared__ typename BlockReduceT::TempStorage temp_storage;
    __shared__ T smem_row_absmax;

    const int row_id = blockIdx.x;
    const T* row_data = A + (row_id * cols);

    // Threads will read the row values in a striped access pattern and find a local absmax.
    T row_local_absmax = -FLT_MIN;
    for (int i = threadIdx.x; i < cols; i += THREADS) {
        const T absval = fabsf(__ldcs(&(row_data[i])));

        // For sparse decomposition, values outside of the threshold are not to be
        // included when calculating the row's absmax.
        if constexpr (SPARSE_DECOMP) {
            row_local_absmax = fmaxf(row_local_absmax, absval < T(threshold) ? absval : row_local_absmax);
        } else {
            row_local_absmax = fmaxf(row_local_absmax, absval);
        }
    }

    // Reduce thread-local absmax across the block.
    const T row_absmax = BlockReduceT(temp_storage).Reduce(row_local_absmax, CUB_REDUCTIONOP_MAX, cols);
    if (threadIdx.x == 0) {
        // Save our block's absmax to shared memory for the quantization step.
        rowStats[row_id] = smem_row_absmax = row_absmax;
    }
    __syncthreads();

    // Quantize row-wise.
    const float scale = __fdividef(127.0f, smem_row_absmax);
    for (int i = threadIdx.x; i < cols; i += THREADS) {
        float val = row_data[i];

        if constexpr (SPARSE_DECOMP) {
            // For sparse decomposition, we do not want to quantize the outliers.
            // Instead they're zeroed out.
            out[row_id * cols + i] = fabs(val) < threshold ? __float2int_rn(val * scale) : 0;
        } else {
            out[row_id * cols + i] = __float2int_rn(val * scale);
        }
    }
}

template __global__ void kInt8VectorQuant<half, 1024, 0>(
    half* __restrict__ A, int8_t* out, float* rowStats, float threshold, int rows, int cols
);
template __global__ void kInt8VectorQuant<half, 1024, 1>(
    half* __restrict__ A, int8_t* out, float* rowStats, float threshold, int rows, int cols
);

#define MM_DEQUANT_CONST 6.200012e-05f // 1.0f/(127.0f*127.0f)

template <int ITEMS_PER_THREAD, int THREADS>
__global__ void kdequant_mm_int32_fp16(
    int* __restrict__ const A, float* __restrict__ const rowStats, float* __restrict__ const colStats, half* out,
    half* __restrict__ const bias, const int numRows, const int numCols, const int n
) {
    const int n_out = numRows * numCols;

    int block_offset = blockIdx.x * THREADS * ITEMS_PER_THREAD;
    int thread_offset = threadIdx.x * ITEMS_PER_THREAD;

    int local_values[ITEMS_PER_THREAD];
    half local_output[ITEMS_PER_THREAD];

    float local_rowStats[ITEMS_PER_THREAD];
    float local_colStats[ITEMS_PER_THREAD];
    float local_biasValue[ITEMS_PER_THREAD];

    typedef cub::BlockLoad<int, THREADS, ITEMS_PER_THREAD, cub::BLOCK_LOAD_VECTORIZE> LoadInt32;
    __shared__ typename LoadInt32::TempStorage loadint32;

    int row_idx, col_idx;

#pragma unroll ITEMS_PER_THREAD
    for (int j = 0; j < ITEMS_PER_THREAD; ++j) {

        row_idx = (block_offset + thread_offset + j) / numCols;
        col_idx = (block_offset + thread_offset + j) % numCols;

        local_colStats[j] = col_idx >= numCols ? 0.0f : __ldg(&colStats[col_idx]);
        local_rowStats[j] = row_idx >= numRows ? 0.0f : __ldg(&rowStats[row_idx]);
        local_biasValue[j] = ((bias == nullptr) || col_idx >= numCols) ? 0.0f : __half2float(bias[col_idx]);
    }

    // Each block loads THREADS * ITEMS_PER_THREAD values from A
    int valid_items =
        block_offset + THREADS * ITEMS_PER_THREAD < n_out ? THREADS * ITEMS_PER_THREAD : n_out - block_offset;
    LoadInt32(loadint32).Load(&(A[block_offset]), local_values, valid_items, 0);

#pragma unroll ITEMS_PER_THREAD
    for (int j = 0; j < ITEMS_PER_THREAD; ++j) {
        local_output[j] = __float2half(
            fmaf(local_values[j] * local_rowStats[j] * local_colStats[j], MM_DEQUANT_CONST, local_biasValue[j])
        );
    }

#pragma unroll ITEMS_PER_THREAD
    for (int j = 0; j < ITEMS_PER_THREAD; j++) {
        int outIdx = block_offset + thread_offset + j;
        if (outIdx < n_out) {
            out[outIdx] = local_output[j];
        }
    }
}

#define num_values_4bit 32

template <typename T, int THREADS, int BITS>
__global__ void kgemm_4bit_inference_naive(
    int M, int N, int K, T* __restrict__ const A, unsigned char* B, float* absmax, const float* datatype, T* out,
    int lda, int ldb, int ldc, int blocksize
) {

    // per threadblock:
    // load step-by-step in chunks of [32,warps]: 1x32 * [32,warps] -> [1,warps]
    // 4 warps -> 4 loads per iter
    // 1x32 * 32x4 -> 1x4 outputs per thread block
    typedef cub::WarpReduce<float> WarpReduce;
    __shared__ typename WarpReduce::TempStorage temp_storage[THREADS / 32];

    const int warp_idx = threadIdx.x / 32;
    const int warp_lane = threadIdx.x % 32;
    const int row_B = (THREADS / 32) * blockIdx.x + warp_idx;
    const int offset_B = ldb * row_B;
    const int num_values_8bit = num_values_4bit / 2;
    float local_C = 0.0f;

    unsigned char local_B_4bit[num_values_8bit];
    T local_B[num_values_4bit / 4];
    T local_A[num_values_4bit / 4];
    __shared__ T quant_map[16];
    T local_absmax = T(0.0f);

    if (threadIdx.x < 16)
        quant_map[threadIdx.x] = T(__ldg(&datatype[threadIdx.x]));
    // for(int i = threadIdx.x; i < 16; i++)
    // quant_map[i] = T(__ldg(&datatype[i]));
    __syncthreads();

    // A: [1, K]
    // B: [N, K]
    for (int inner_idx = warp_lane * num_values_4bit; inner_idx < K; inner_idx += 32 * num_values_4bit) {
        const int inner_idx_halved = inner_idx / 2;

        // Since blocksize will always be a power-of-2, we avoid more expensive
        // division by the blocksize and instead use a shift operation.
        // This is equivalent to (i+threadId.x*NUM_PER_TH)/blocksize.
        const int absidx = ((2 * offset_B) + inner_idx) >> (31 - __clz(blocksize));

        local_absmax = __ldg(&(absmax[absidx]));

        if (row_B < M) {
            if ((inner_idx_halved + num_values_8bit) < (K / 2)) {
                // this is the most important for performance considerations
                reinterpret_cast<int4(&)[num_values_8bit]>(local_B_4bit)[0] =
                    reinterpret_cast<int4*>(B)[(offset_B + (inner_idx_halved)) / (num_values_8bit)];
            } else {
#pragma unroll
                for (int j = 0; j < (num_values_8bit); j++)
                    if ((inner_idx_halved) + j < (K / 2))
                        local_B_4bit[j] = B[offset_B + inner_idx_halved + j];
                    else
                        local_B_4bit[j] = 0b01110111;
            }
        } else {
#pragma unroll
            for (int j = 0; j < (num_values_8bit); j++)
                local_B_4bit[j] = 0b01110111;
        }

        for (int i = 0; i < 4; i++) {
#pragma unroll
            for (int k = 0; k < num_values_8bit / 4; k++) {
#if BNB_BF16_AVAILABLE
                local_B[k * 2] = quant_map[local_B_4bit[(i * num_values_8bit / 4) + k] >> 4] * local_absmax;
                local_B[k * 2 + 1] = quant_map[local_B_4bit[(i * num_values_8bit / 4) + k] & 0x0F] * local_absmax;
#else
                // bf16 multipliation not supported
                local_B[k * 2] =
                    T((float)quant_map[local_B_4bit[(i * num_values_8bit / 4) + k] >> 4] * (float)local_absmax);
                local_B[k * 2 + 1] =
                    T((float)quant_map[local_B_4bit[(i * num_values_8bit / 4) + k] & 0x0F] * (float)local_absmax);
#endif
            }

            if (inner_idx + (num_values_4bit / 4) + (i * num_values_4bit / 4) < K) {
                // this is also relatively important for performance
                if (BITS == 16) {
                    reinterpret_cast<int4(&)[num_values_4bit]>(local_A)[0] =
                        reinterpret_cast<int4*>(A)[inner_idx / (num_values_4bit / 4) + i];
                } else {
                    reinterpret_cast<int4(&)[num_values_4bit]>(local_A)[0] =
                        reinterpret_cast<int4*>(A)[inner_idx / (num_values_4bit / 8) + (2 * i) + 0];
                    reinterpret_cast<int4(&)[num_values_4bit]>(local_A)[1] =
                        reinterpret_cast<int4*>(A)[inner_idx / (num_values_4bit / 8) + (2 * i) + 1];
                }

            } else
#pragma unroll
                for (int k = 0; k < num_values_4bit / 4; k++)
                    if (inner_idx + (i * num_values_4bit / 4) + k < K)
                        local_A[k] = A[inner_idx + k + (i * num_values_4bit / 4)];
                    else
                        local_A[k] = T(0.0f);

// accumulate in float; small performance hit for Ampere, but lower error for outputs
#pragma unroll
            for (int k = 0; k < num_values_4bit / 4; k++) {
#if BNB_BF16_AVAILABLE
                local_C += (float)(local_A[k] * local_B[k]);
#else
                // bf16 multipliation not supported
                local_C += ((float)local_A[k] * (float)local_B[k]);
#endif
            }
        }
    }

    local_C = WarpReduce(temp_storage[warp_idx]).Sum(local_C);

    if (row_B < M && warp_lane == 0)
        out[row_B] = T(local_C);
}

template <typename T, int FUNC> __global__ void kfunc(T* A, T* B, T value, long n) {
    for (long i = (blockDim.x * blockIdx.x) + threadIdx.x; i < n; i += (blockDim.x * gridDim.x)) {
        switch (FUNC) {
        case FILL:
            A[i] = (T)value;
            break;
        case ARANGE:
            A[i] = (T)i;
            break;
        case _MUL:
            A[i] = A[i] * B[i];
            break;
        }
    }
}

//==============================================================
//                   TEMPLATE DEFINITIONS
//==============================================================

template __global__ void kfunc<float, FILL>(float* A, float* B, float value, long n);
template __global__ void kfunc<unsigned char, FILL>(unsigned char* A, unsigned char* B, unsigned char value, long n);
template __global__ void kfunc<float, ARANGE>(float* A, float* B, float value, long n);
template __global__ void kfunc<float, _MUL>(float* A, float* B, float value, long n);

template __global__ void kgemm_4bit_inference_naive<half, 128, 16>(
    int M, int N, int K, half* __restrict__ const A, unsigned char* B, float* absmax, const float* datatype, half* out,
    int lda, int ldb, int ldc, int blocksize
);
template __global__ void kgemm_4bit_inference_naive<__nv_bfloat16, 128, 16>(
    int M, int N, int K, __nv_bfloat16* __restrict__ const A, unsigned char* B, float* absmax, const float* datatype,
    __nv_bfloat16* out, int lda, int ldb, int ldc, int blocksize
);
template __global__ void kgemm_4bit_inference_naive<float, 128, 32>(
    int M, int N, int K, float* __restrict__ const A, unsigned char* B, float* absmax, const float* datatype,
    float* out, int lda, int ldb, int ldc, int blocksize
);

template __global__ void kdequant_mm_int32_fp16<4, 512>(
    int* __restrict__ const A, float* __restrict__ const rowStats, float* __restrict__ const colStats, half* out,
    half* __restrict__ const bias, const int numRows, const int numCols, const int n
);

template __device__ unsigned char dQuantize<0>(float* smem_code, const float rand, float x);
template __device__ unsigned char dQuantize<1>(float* smem_code, const float rand, float x);

#define MAKE_PreconditionOptimizer32bit1State(oname, gtype)                                                            \
    template __global__ void kPreconditionOptimizer32bit1State<gtype, oname, 4096, 8>(                                 \
        gtype * g, gtype * p, float* state1, float* unorm, const float beta1, const float beta2, const float eps,      \
        const float weight_decay, const int step, const float lr, const float gnorm_scale, const int n                 \
    );

MAKE_PreconditionOptimizer32bit1State(MOMENTUM, half)
MAKE_PreconditionOptimizer32bit1State(MOMENTUM, float)
MAKE_PreconditionOptimizer32bit1State(MOMENTUM, __nv_bfloat16)
MAKE_PreconditionOptimizer32bit1State(RMSPROP, half)
MAKE_PreconditionOptimizer32bit1State(RMSPROP, float)
MAKE_PreconditionOptimizer32bit1State(RMSPROP, __nv_bfloat16)
MAKE_PreconditionOptimizer32bit1State(LION, half)
MAKE_PreconditionOptimizer32bit1State(LION, float)
MAKE_PreconditionOptimizer32bit1State(LION, __nv_bfloat16)
MAKE_PreconditionOptimizer32bit1State(ADAGRAD, half)
MAKE_PreconditionOptimizer32bit1State(ADAGRAD, float)
MAKE_PreconditionOptimizer32bit1State(ADAGRAD, __nv_bfloat16)

#define MAKE_Optimizer32bit1State(oname, gtype)                                                                        \
    template __global__ void kOptimizer32bit1State<gtype, oname>(                                                      \
        gtype * g, gtype * p, float* state1, float* unorm, const float max_unorm, const float param_norm,              \
        const float beta1, const float beta2, const float eps, const float weight_decay, const int step,               \
        const float lr, const float gnorm_scale, const bool skip_zeros, const int n                                    \
    );

MAKE_Optimizer32bit1State(MOMENTUM, half)
MAKE_Optimizer32bit1State(MOMENTUM, float)
MAKE_Optimizer32bit1State(MOMENTUM, __nv_bfloat16)
MAKE_Optimizer32bit1State(RMSPROP, half)
MAKE_Optimizer32bit1State(RMSPROP, float)
MAKE_Optimizer32bit1State(RMSPROP, __nv_bfloat16)
MAKE_Optimizer32bit1State(LION, half)
MAKE_Optimizer32bit1State(LION, float)
MAKE_Optimizer32bit1State(LION, __nv_bfloat16)
MAKE_Optimizer32bit1State(ADAGRAD, half)
MAKE_Optimizer32bit1State(ADAGRAD, float)
MAKE_Optimizer32bit1State(ADAGRAD, __nv_bfloat16)

#define MAKE_PreconditionOptimizer32bit2State(oname, gtype)                                                            \
    template __global__ void kPreconditionOptimizer32bit2State<gtype, oname, 4096, 8>(                                 \
        gtype * g, gtype * p, float* state1, float* state2, float* unorm, const float beta1, const float beta2,        \
        const float eps, const float weight_decay, const int step, const float lr, const float gnorm_scale,            \
        const int n                                                                                                    \
    );

MAKE_PreconditionOptimizer32bit2State(ADAM, float)
MAKE_PreconditionOptimizer32bit2State(ADAM, half)
MAKE_PreconditionOptimizer32bit2State(ADAM, __nv_bfloat16)
MAKE_PreconditionOptimizer32bit2State(ADEMAMIX, float)
MAKE_PreconditionOptimizer32bit2State(ADEMAMIX, half)
MAKE_PreconditionOptimizer32bit2State(ADEMAMIX, __nv_bfloat16)

template __global__ void kOptimizer32bit2State<float, ADAM>(
    float* g, float* p, float* state1, float* state2, float* unorm, const float max_unorm, const float param_norm,
    const float beta1, const float beta2, const float beta3, const float alpha, const float eps,
    const float weight_decay, const int step, const float lr, const float gnorm_scale, const bool skip_zeros,
    const int n
);
template __global__ void kOptimizer32bit2State<half, ADAM>(
    half* g, half* p, float* state1, float* state2, float* unorm, const float max_unorm, const float param_norm,
    const float beta1, const float beta2, const float beta3, const float alpha, const float eps,
    const float weight_decay, const int step, const float lr, const float gnorm_scale, const bool skip_zeros,
    const int n
);
template __global__ void kOptimizer32bit2State<__nv_bfloat16, ADAM>(
    __nv_bfloat16* g, __nv_bfloat16* p, float* state1, float* state2, float* unorm, const float max_unorm,
    const float param_norm, const float beta1, const float beta2, const float beta3, const float alpha, const float eps,
    const float weight_decay, const int step, const float lr, const float gnorm_scale, const bool skip_zeros,
    const int n
);
template __global__ void kOptimizer32bit2State<float, ADEMAMIX>(
    float* g, float* p, float* state1, float* state2, float* unorm, const float max_unorm, const float param_norm,
    const float beta1, const float beta2, const float beta3, const float alpha, const float eps,
    const float weight_decay, const int step, const float lr, const float gnorm_scale, const bool skip_zeros,
    const int n
);
template __global__ void kOptimizer32bit2State<half, ADEMAMIX>(
    half* g, half* p, float* state1, float* state2, float* unorm, const float max_unorm, const float param_norm,
    const float beta1, const float beta2, const float beta3, const float alpha, const float eps,
    const float weight_decay, const int step, const float lr, const float gnorm_scale, const bool skip_zeros,
    const int n
);
template __global__ void kOptimizer32bit2State<__nv_bfloat16, ADEMAMIX>(
    __nv_bfloat16* g, __nv_bfloat16* p, float* state1, float* state2, float* unorm, const float max_unorm,
    const float param_norm, const float beta1, const float beta2, const float beta3, const float alpha, const float eps,
    const float weight_decay, const int step, const float lr, const float gnorm_scale, const bool skip_zeros,
    const int n
);

#define MAKE_kQuantizeBlockwise(dtype, blocksize, num_per_thread, stochastic, data_type_name)                          \
    template __global__ void kQuantizeBlockwise<dtype, blocksize, num_per_thread, stochastic, data_type_name>(         \
        float* code, dtype* __restrict__ const A, float* absmax, unsigned char* out, float* __restrict__ const rand,   \
        const int rand_offset, const int n                                                                             \
    );

MAKE_kQuantizeBlockwise(half, 4096, 4, 0, General8bit)
MAKE_kQuantizeBlockwise(half, 4096, 4, 1, General8bit)
MAKE_kQuantizeBlockwise(half, 2048, 4, 0, General8bit)
MAKE_kQuantizeBlockwise(half, 1024, 4, 0, General8bit)
MAKE_kQuantizeBlockwise(half, 512, 2, 0, General8bit)
MAKE_kQuantizeBlockwise(half, 256, 2, 0, General8bit)
MAKE_kQuantizeBlockwise(half, 128, 2, 0, General8bit)
MAKE_kQuantizeBlockwise(half, 64, 2, 0, General8bit)
MAKE_kQuantizeBlockwise(half, 4096, 4, 0, FP4)
MAKE_kQuantizeBlockwise(half, 2048, 4, 0, FP4)
MAKE_kQuantizeBlockwise(half, 1024, 4, 0, FP4)
MAKE_kQuantizeBlockwise(half, 512, 2, 0, FP4)
MAKE_kQuantizeBlockwise(half, 256, 2, 0, FP4)
MAKE_kQuantizeBlockwise(half, 128, 2, 0, FP4)
MAKE_kQuantizeBlockwise(half, 64, 2, 0, FP4)
MAKE_kQuantizeBlockwise(half, 4096, 4, 0, NF4)
MAKE_kQuantizeBlockwise(half, 2048, 4, 0, NF4)
MAKE_kQuantizeBlockwise(half, 1024, 4, 0, NF4)
MAKE_kQuantizeBlockwise(half, 512, 2, 0, NF4)
MAKE_kQuantizeBlockwise(half, 256, 2, 0, NF4)
MAKE_kQuantizeBlockwise(half, 128, 2, 0, NF4)
MAKE_kQuantizeBlockwise(half, 64, 2, 0, NF4)
MAKE_kQuantizeBlockwise(float, 4096, 4, 0, General8bit)
MAKE_kQuantizeBlockwise(float, 4096, 4, 1, General8bit)
MAKE_kQuantizeBlockwise(float, 2048, 4, 0, General8bit)
MAKE_kQuantizeBlockwise(float, 1024, 4, 0, General8bit)
MAKE_kQuantizeBlockwise(float, 512, 2, 0, General8bit)
MAKE_kQuantizeBlockwise(float, 256, 2, 0, General8bit)
MAKE_kQuantizeBlockwise(float, 128, 2, 0, General8bit)
MAKE_kQuantizeBlockwise(float, 64, 2, 0, General8bit)
MAKE_kQuantizeBlockwise(float, 4096, 4, 0, FP4)
MAKE_kQuantizeBlockwise(float, 2048, 4, 0, FP4)
MAKE_kQuantizeBlockwise(float, 1024, 4, 0, FP4)
MAKE_kQuantizeBlockwise(float, 512, 2, 0, FP4)
MAKE_kQuantizeBlockwise(float, 256, 2, 0, FP4)
MAKE_kQuantizeBlockwise(float, 128, 2, 0, FP4)
MAKE_kQuantizeBlockwise(float, 64, 2, 0, FP4)
MAKE_kQuantizeBlockwise(float, 4096, 4, 0, NF4)
MAKE_kQuantizeBlockwise(float, 2048, 4, 0, NF4)
MAKE_kQuantizeBlockwise(float, 1024, 4, 0, NF4)
MAKE_kQuantizeBlockwise(float, 512, 2, 0, NF4)
MAKE_kQuantizeBlockwise(float, 256, 2, 0, NF4)
MAKE_kQuantizeBlockwise(float, 128, 2, 0, NF4)
MAKE_kQuantizeBlockwise(float, 64, 2, 0, NF4)

MAKE_kQuantizeBlockwise(__nv_bfloat16, 4096, 4, 0, General8bit)
MAKE_kQuantizeBlockwise(__nv_bfloat16, 4096, 4, 1, General8bit)
MAKE_kQuantizeBlockwise(__nv_bfloat16, 2048, 4, 0, General8bit)
MAKE_kQuantizeBlockwise(__nv_bfloat16, 1024, 4, 0, General8bit)
MAKE_kQuantizeBlockwise(__nv_bfloat16, 512, 2, 0, General8bit)
MAKE_kQuantizeBlockwise(__nv_bfloat16, 256, 2, 0, General8bit)
MAKE_kQuantizeBlockwise(__nv_bfloat16, 128, 2, 0, General8bit)
MAKE_kQuantizeBlockwise(__nv_bfloat16, 64, 2, 0, General8bit)
MAKE_kQuantizeBlockwise(__nv_bfloat16, 4096, 4, 0, FP4)
MAKE_kQuantizeBlockwise(__nv_bfloat16, 2048, 4, 0, FP4)
MAKE_kQuantizeBlockwise(__nv_bfloat16, 1024, 4, 0, FP4)
MAKE_kQuantizeBlockwise(__nv_bfloat16, 512, 2, 0, FP4)
MAKE_kQuantizeBlockwise(__nv_bfloat16, 256, 2, 0, FP4)
MAKE_kQuantizeBlockwise(__nv_bfloat16, 128, 2, 0, FP4)
MAKE_kQuantizeBlockwise(__nv_bfloat16, 64, 2, 0, FP4)
MAKE_kQuantizeBlockwise(__nv_bfloat16, 4096, 4, 0, NF4)
MAKE_kQuantizeBlockwise(__nv_bfloat16, 2048, 4, 0, NF4)
MAKE_kQuantizeBlockwise(__nv_bfloat16, 1024, 4, 0, NF4)
MAKE_kQuantizeBlockwise(__nv_bfloat16, 512, 2, 0, NF4)
MAKE_kQuantizeBlockwise(__nv_bfloat16, 256, 2, 0, NF4)
MAKE_kQuantizeBlockwise(__nv_bfloat16, 128, 2, 0, NF4)
MAKE_kQuantizeBlockwise(__nv_bfloat16, 64, 2, 0, NF4)

// Template instantiations for blocksize=32 specialized kernel (4-bit only)
#define MAKE_kQuantizeBlockwise32(dtype, data_type_name)                                                               \
    template __global__ void kQuantizeBlockwise32<dtype, data_type_name>(                                              \
        float* code, dtype* __restrict__ const A, float* absmax, unsigned char* out, float* __restrict__ const rand,   \
        const int rand_offset, const int n                                                                             \
    );

// FP4 instantiations for blocksize=32
MAKE_kQuantizeBlockwise32(half, FP4) MAKE_kQuantizeBlockwise32(float, FP4) MAKE_kQuantizeBlockwise32(__nv_bfloat16, FP4)

    // NF4 instantiations for blocksize=32
    MAKE_kQuantizeBlockwise32(half, NF4) MAKE_kQuantizeBlockwise32(float, NF4) MAKE_kQuantizeBlockwise32(
        __nv_bfloat16, NF4
    )

        template __global__ void kDequantizeBlockwise<half, 512, 64, 8, FP4>(
            float* code, unsigned char* A, float* absmax, half* out, const int blocksize, const int n
        );
template __global__ void kDequantizeBlockwise<half, 512, 64, 8, General8bit>(
    float* code, unsigned char* A, float* absmax, half* out, const int blocksize, const int n
);
template __global__ void kDequantizeBlockwise<half, 512, 64, 8, NF4>(
    float* code, unsigned char* A, float* absmax, half* out, const int blocksize, const int n
);
template __global__ void kDequantizeBlockwise<float, 512, 64, 8, FP4>(
    float* code, unsigned char* A, float* absmax, float* out, const int blocksize, const int n
);
template __global__ void kDequantizeBlockwise<float, 512, 64, 8, General8bit>(
    float* code, unsigned char* A, float* absmax, float* out, const int blocksize, const int n
);
template __global__ void kDequantizeBlockwise<float, 512, 64, 8, NF4>(
    float* code, unsigned char* A, float* absmax, float* out, const int blocksize, const int n
);
template __global__ void kDequantizeBlockwise<__nv_bfloat16, 512, 64, 8, FP4>(
    float* code, unsigned char* A, float* absmax, __nv_bfloat16* out, const int blocksize, const int n
);
template __global__ void kDequantizeBlockwise<__nv_bfloat16, 512, 64, 8, General8bit>(
    float* code, unsigned char* A, float* absmax, __nv_bfloat16* out, const int blocksize, const int n
);
template __global__ void kDequantizeBlockwise<__nv_bfloat16, 512, 64, 8, NF4>(
    float* code, unsigned char* A, float* absmax, __nv_bfloat16* out, const int blocksize, const int n
);

#define MAKE_OptimizerStatic8bit2StateBlockwise(oname, gtype, block_size, num_per_thread)                              \
    template __global__ void kOptimizerStatic8bit2StateBlockwise<gtype, oname, block_size, num_per_thread>(            \
        gtype * p, gtype* __restrict__ const g, unsigned char* state1, unsigned char* state2, const float beta1,       \
        const float beta2, const float beta3, const float alpha, const float eps, const int step, const float lr,      \
        float* __restrict__ const quantiles1, float* __restrict__ const quantiles2, float* absmax1, float* absmax2,    \
        float weight_decay, const float gnorm_scale, const bool skip_zeros, const int n                                \
    );

MAKE_OptimizerStatic8bit2StateBlockwise(ADAM, float, 256, 1)
MAKE_OptimizerStatic8bit2StateBlockwise(ADAM, half, 256, 1)
MAKE_OptimizerStatic8bit2StateBlockwise(ADAM, __nv_bfloat16, 256, 1)
MAKE_OptimizerStatic8bit2StateBlockwise(ADEMAMIX, float, 256, 1)
MAKE_OptimizerStatic8bit2StateBlockwise(ADEMAMIX, half, 256, 1)
MAKE_OptimizerStatic8bit2StateBlockwise(ADEMAMIX, __nv_bfloat16, 256, 1)

#define MAKE_OptimizerStatic8bit1StateBlockwise(oname, gtype, block_size, num_per_thread)                              \
    template __global__ void kOptimizerStatic8bit1StateBlockwise<gtype, oname, block_size, num_per_thread>(            \
        gtype * p, gtype* __restrict__ const g, unsigned char* state1, const float beta1, const float beta2,           \
        const float eps, const int step, const float lr, float* __restrict__ const quantiles1, float* absmax1,         \
        float weight_decay, const float gnorm_scale, const bool skip_zeros, const int n                                \
    );

MAKE_OptimizerStatic8bit1StateBlockwise(MOMENTUM, float, 256, 1)
MAKE_OptimizerStatic8bit1StateBlockwise(MOMENTUM, half, 256, 1)
MAKE_OptimizerStatic8bit1StateBlockwise(MOMENTUM, __nv_bfloat16, 256, 1)
MAKE_OptimizerStatic8bit1StateBlockwise(RMSPROP, float, 256, 1)
MAKE_OptimizerStatic8bit1StateBlockwise(RMSPROP, half, 256, 1)
MAKE_OptimizerStatic8bit1StateBlockwise(RMSPROP, __nv_bfloat16, 256, 1)
MAKE_OptimizerStatic8bit1StateBlockwise(LION, float, 256, 1)
MAKE_OptimizerStatic8bit1StateBlockwise(LION, half, 256, 1)
MAKE_OptimizerStatic8bit1StateBlockwise(LION, __nv_bfloat16, 256, 1)
MAKE_OptimizerStatic8bit1StateBlockwise(ADAGRAD, float, 256, 1)
MAKE_OptimizerStatic8bit1StateBlockwise(ADAGRAD, half, 256, 1)
MAKE_OptimizerStatic8bit1StateBlockwise(ADAGRAD, __nv_bfloat16, 256, 1)
